import nltk 
nltk.download('twitter_samples')
nltk.download('stopwords')
from nltk.corpus import twitter_samples       # ä»nltkè¯­æ–™åº“å¯¼å…¥twitteræ•°æ®é›†
from nltk.corpus import stopwords    # å¯¼å…¥åœç”¨è¯
from nltk.stem import PorterStemmer   # å¯¼å…¥è¯å¹²æ¨¡å—
from nltk.tokenize import TweetTokenizer       # å¯¼å…¥åˆ†è¯æ¨¡å—

import pandas as pd
import numpy as np
import re
import string

'''
  æ•°æ®é¢„å¤„ç†
'''

# å°†æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œè¿‡æ»¤ä¸éœ€è¦çš„æ•°æ®
def process_tweet(tweet):
    stemmer = PorterStemmer()
    stopwords_english = stopwords.words('english')
    # å»é™¤ç¬¦å·
    tweet = re.sub(r'\$\w*','',tweet)
    tweet = re.sub(r'^RT[\s]+','',tweet)
    tweet = re.sub(r'https?:\/\/.*[\r\n]*','', tweet)
    tweet = re.sub(r'#', '', tweet)

    # è¿›è¡Œåˆ†è¯
    tokenizer=TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)
    tweet_tokens =tokenizer.tokenize(tweet)

    tweet_clean =[]
    for token in tweet_tokens:
        if token not in stopwords_english and token not in string.punctuation:
            stem_token = stemmer.stem(token)
            tweet_clean.append(token)
    
    return tweet_clean

# è®¡ç®—æ¯ä¸ªå•è¯åˆ†åˆ«åœ¨positiv å’Œnegative ä¸¤ç§æƒ…å†µä¸‹å‡ºç°çš„é¢‘ç‡
def build_freqs(tweets,label_array):
    """
    Input:
    tweets: tweetsæ•°æ®åˆ—è¡¨
    label_array : ä¸€ä¸ªå¸¦æœ‰æƒ…æ„Ÿæ ‡ç­¾çš„æ•°ç»„

    outputï¼š
    freqsï¼šå°†æ¯å¯¹ï¼ˆå•è¯ï¼Œæƒ…æ„Ÿï¼‰ æ˜ å°„åˆ°å…¶çš„å­—å…¸é¢‘ç‡
    """
    # å°†label_array çš„æ•°ç»„è½¬åŒ–ä¸ºåˆ—è¡¨ ï¼Œå’Œtweets å¯¹åº”
    label_array_list = np.squeeze(label_array).tolist()

    freqs = {}
    for label, tweet in zip(label_array_list,tweets):
        for word in process_tweet(tweet):
            pair = (word,label)
            if pair in freqs:
                freqs[pair]+=1
            else:
                freqs[pair]=1
    return freqs




'''
å‡½æ•°æ¨¡å‹
'''
# æ¿€æ´»å‡½æ•°sigmoid
def sigmoid(x,theta):
    '''
    Input:
        x: a feature vector of dimension (1,n+1)
        theta: your final weight vector
    Output:
        h: the sigmoid of z
    '''

    h = 1/(1+np.exp(-(x.dot(theta))))

    return h


# æ¢¯åº¦ä¸‹é™
def gradientDescent(x, y, theta, alpha, num_iters):
    '''
    Input:
        x: matrix of features which is (m,n+1)
        y: corresponding labels of the input matrix x, dimensions (m,1)
        theta: weight vector of dimension (n+1,1)
        alpha: learning rate
        num_iters: number of iterations you want to train your model for
    Output:
        J: the final cost
        theta: your final weight vector
    Hint: you might want to print the cost to make sure that it is going down.
    '''
    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###
    # get 'm', the number of rows in matrix x
    m = np.shape(x)[0]
    
    for i in range(0, num_iters):
        
        
        
        
        # get the sigmoid of z
        h =  sigmoid(x,theta)
        
        # calculate the cost function
        J = (-1) * (y.T.dot(np.log(h)) + ((1-y).T).dot(np.log(1-h))) / m

        # update the weights theta
        theta = theta - (alpha*(x.T.dot(h-y))) / m
        
    ### END CODE HERE ###
    #J = float(J)
    return theta


'''
æå–ç‰¹å¾:ç»Ÿè®¡ä¸€å¥tweetä¸­ï¼Œpositive å’Œnegative çš„æ•°é‡ï¼Œå½¢æˆä¸€ä¸ªå‘é‡
'''

def extract_features(tweet,ferqs):
    '''
    Input: 
        tweet: a list of words for one tweet
        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)
    Output: 
        x: a feature vector of dimension (1,3)
    '''

    words = process_tweet(tweet)

    x = np.zeros((1,3))
    x[0,0] = 1

    for word in words:

        if (word,1) in freqs.keys():
            x[0,1] += freqs[(word,1)]

        elif (word,0) in freqs.keys():
            x[0,2] += freqs[(word,0)]

    assert(x.shape==(1,3))

    return x


'''
   æ¨¡å‹å‡†ç¡®åº¦è®­ç»ƒ
'''

def predict(tweet,freqs,theta):
    '''
    ğ‘¦_ğ‘ğ‘Ÿğ‘’ğ‘‘=ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ±â‹…ğœƒ)
    Input: 
        tweet: a string
        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)
        theta: (n+1,1) vector of weights
    Output: 
        y_pred: the probability of a tweet being positive or negative
    '''
    x =  extract_features(tweet,freqs)

    y_pred = sigmoid(x,theta)

    return y_pred


def test_logistic_regression(test_tweets,test_labels,freqs,theta):
    """
    Input: 
        test_tweets: a list of tweets
        test_labels: (m, 1) vector with the corresponding labels for the list of tweets
        freqs: a dictionary with the frequency of each pair (or tuple)
        theta: weight vector of dimension (3, 1)
    Output: 
        accuracy: (# of tweets classified correctly) / (total # of tweets)
    """
    
    y_hat =[]
    

    for tweet in test_tweets:
        y_pred = predict(tweet,freqs,theta)

        if y_pred >0.5:
            y_hat.append(1)
        else:
            y_hat.append(0)

    j = 0
    for i in range(len(y_hat)):
        if  np.asarray(y_hat)[i] == np.squeeze(test_labels)[i]:
            j +=1

    accuracy  = j/len(y_hat)
    return accuracy

if __name__ == "__main__":
    
    
    '''
    åˆå§‹åŒ–æ•°æ®
    '''


    all_positive_tweets = twitter_samples.strings('positive_tweets.json')
    all_negative_tweets = twitter_samples.strings('negative_tweets.json')


    #print(len(all_positive_tweets))

    # å°†æ•°æ®é›†æŒ‰ç…§8/2 çš„æ¯”ä¾‹ åˆ†é… è®­ç»ƒé›†å’Œæµ‹è¯•é›† 

    train_positive_tweets = all_positive_tweets[:4000]
    test_positive_tweets =all_positive_tweets[4000:]

    train_negative_tweets = all_negative_tweets[:4000]
    test_negative_tweets =all_negative_tweets[4000:]

    train_tweets = train_positive_tweets + train_negative_tweets
    test_tweets = test_positive_tweets + test_negative_tweets



    # åˆ›å»ºpositiveæ ‡ç­¾å’Œnegativeæ ‡ç­¾çš„numpyæ•°ç»„
    train_labels =np.append(np.ones((len(train_positive_tweets),1)),np.zeros((len(train_negative_tweets),1)),axis=0)
    test_labels =np.append(np.ones((len(test_positive_tweets),1)),np.zeros((len(test_negative_tweets),1)),axis=0)

    #åˆ›å»ºé¢‘ç‡å­—å…¸
    freqs = build_freqs(train_tweets,train_labels)

    X = np.zeros((len(train_tweets),3))
    for i in range(len(train_tweets)):
        X[i,:]=extract_features(train_tweets[i],freqs)
        
    Y = train_labels

    num_iters  = int(input('è¯·è¾“å…¥è¿­ä»£æ¬¡æ•°ï¼š'))
    for i in range(num_iters):

        theta = gradientDescent(X,Y,np.zeros((3,1)),1e-9,i)
        #print(theta)
        accuracy = test_logistic_regression(test_tweets,test_labels,freqs,theta)
        if i % 50 == 0:
            print('ç¬¬%dæ¬¡çš„å‡†ç¡®ç‡ä¸º%f'%(i,accuracy))
    


